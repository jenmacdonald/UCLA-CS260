{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2 -- BMLCO: Solving Real ML Problems\n",
    "\n",
    "## CS260 Winter 2018\n",
    "## Machine Learning Algorithms\n",
    "## Dr. Yutao He"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Due: March 25th, 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Name: Jennifer MacDonald\n",
    "### SID: 604501712"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Study the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Standard scientific Python imports\n",
    "import numpy as np\n",
    "\n",
    "# Import performance metrics\n",
    "from sklearn import metrics\n",
    "\n",
    "# Import classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Import feature selctor and cross validator\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The \"Smartphone-Based Recognition of Human Activities and Postural Transitions \n",
    "# Data Set\"  dataset\n",
    "X_train = np.genfromtxt('HAPT Data Set/Train/X_train.txt')\n",
    "y_train = np.genfromtxt('HAPT Data Set/Train/y_train.txt')\n",
    "\n",
    "X_test = np.genfromtxt('HAPT Data Set/Test/X_test.txt')\n",
    "y_test = np.genfromtxt('HAPT Data Set/Test/y_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The data that we are interested in are 10929 instances from 30 volunteers \n",
    "#recorded on a smart device. Each instance consists of measurements of 42 different\n",
    "# attributes. The combination of these attributes can be used to build a model that \n",
    "# can predict six basic activities that include three static postures (standing, \n",
    "# sitting, lying) and three dynamic activities (walking, walking downstairs, and \n",
    "# walking upstairs). Data about transitions between the postures or activities were\n",
    "# also recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Sample: \n",
      " [[ 0.04357967 -0.00597022 -0.03505434 ..., -0.84155851  0.17991281\n",
      "  -0.05171842]\n",
      " [ 0.03948004 -0.00213128 -0.02906736 ..., -0.8450924   0.18026111\n",
      "  -0.04743634]\n",
      " [ 0.03997778 -0.00515272 -0.02265071 ..., -0.84923013  0.18060956\n",
      "  -0.04227136]\n",
      " [ 0.03978456 -0.01180878 -0.02891578 ..., -0.84894659  0.18190709\n",
      "  -0.04082622]]\n",
      "Test Data Sample: \n",
      " [ 5.  5.  5.  5.]\n"
     ]
    }
   ],
   "source": [
    "# Here is a look at the first 4 instances from the training set, as well as the\n",
    "# first 4 results.\n",
    "print(\"Training Data Sample: \\n\", X_train[0:4])\n",
    "print(\"Test Data Sample: \\n\", y_train[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Select the feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Attributes: \n",
      " 561\n"
     ]
    }
   ],
   "source": [
    "# Here is the length of each inner array corresponding to the attributes\n",
    "# measured.\n",
    "print(\"Number of Attributes: \\n\", len(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 561 attributes seems like a lot. We can narrow it down to the 17 most\n",
    "# important features using Variance threshold feature selection to remove the\n",
    "# features with low variance. In this case, we're removing all features whose \n",
    "# variance isn't at least 65%.\n",
    "\n",
    "# sel = VarianceThreshold(threshold=(.65 * (1 - .65)))\n",
    "# X_train = sel.fit_transform(X_train)\n",
    "# X_test = sel.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the new length of each inner array corresponding to the attributes\n",
    "# measured.\n",
    "\n",
    "# print(\"Number of Attributes in Training Data After Feature Selection: \\n\", len(X_train[0]))\n",
    "# print(\"Number of Attributes in Testing Data After Feature Selection: \\n\", len(X_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Instances in the Training Data Set: \n",
      " 7767\n",
      "Number of Instances in the Testing Data Set: \n",
      " 3162\n"
     ]
    }
   ],
   "source": [
    "# Here is the length of the outer array corresponding the number of instances\n",
    "# in the training and testing sets.\n",
    "print(\"Number of Instances in the Training Data Set: \\n\", len(X_train))\n",
    "print(\"Number of Instances in the Testing Data Set: \\n\", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Develop the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a classifier: a KNN classifier \n",
    "knn = KNeighborsClassifier(n_neighbors=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a classifier: a Naive Bayes classifier \n",
    "nb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a classifier: a Decision Trees classifier \n",
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a classifiers: a linear SVM classifier generalized to the multi-class\n",
    "# case through a One-Vs-All (OVA) approach. This model was also selected in\n",
    "# addition to the project's required classifiers because there is a published\n",
    "# study that used this classifier on the same data set, and I'm interested to\n",
    "# see if my results mirror the one in their paper.\n",
    "lsvc = LinearSVC(multi_class=\"ovr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=6, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Validate and evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_knn = y_test\n",
    "predicted_knn = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885515496521\n"
     ]
    }
   ],
   "source": [
    "knn_accuracy = np.sum(predicted_knn==expected_knn)/len(expected_knn)\n",
    "print(knn_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation Accuracy, Check for Over/Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "knn_scores = cross_val_score(knn, X_train, y_train)\n",
    "print(\"%0.2f (+/- %0.2f)\" % (knn_scores.mean(), knn_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[487  46  53   0   0   0   1   0   0   0   2   1]\n",
      " [  1 419  51   4   2   1   3   0   0   0   4   0]\n",
      " [  8   6 316   0   0   0   0   0   0   0   1   0]\n",
      " [  0   0   0 431  59   2   1   0   0   0   0   0]\n",
      " [  0   0   0  73 495   2   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0 540   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0   0  18   0   0   0   2   0]\n",
      " [  0   0   0   0   0   0   0  10   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  29   0  15   1]\n",
      " [  0   0   0   0   0   0   0   0   0  22   0  13]\n",
      " [  0   0   0   0   0   0   0   0   3   1  23   2]\n",
      " [  0   0   0   0   0   0   0   0   0   2   0  10]]\n"
     ]
    }
   ],
   "source": [
    "knn_confusion_matrix = metrics.confusion_matrix(predicted_knn, expected_knn)\n",
    "print(knn_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_nb = y_test\n",
    "predicted_nb = nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.747311827957\n"
     ]
    }
   ],
   "source": [
    "nb_accuracy = np.sum(predicted_nb==expected_nb)/len(expected_nb)\n",
    "print(nb_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation Accuracy, Check for Over/Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71 (+/- 0.07)\n"
     ]
    }
   ],
   "source": [
    "nb_scores = cross_val_score(nb, X_train, y_train)\n",
    "print(\"%0.2f (+/- %0.2f)\" % (nb_scores.mean(), nb_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[416   8  80   0   0   0   0   0   0   0   0   0]\n",
      " [ 38 442  83   0   0   0   1   0   0   0   0   0]\n",
      " [ 42  11 257   0   0   0   1   0   0   0   0   0]\n",
      " [  0   0   0 457 311  62   0   0   0   0   0   0]\n",
      " [  0   0   0  35 220   0   0   0   0   0   1   0]\n",
      " [  0   0   0   1   1 467   0   0   0   0   0   0]\n",
      " [  0   9   0   8  22   0  15   0   0   0   1   1]\n",
      " [  0   0   0   5   0   0   2   9   1   0   0   0]\n",
      " [  0   0   0   1   0   0   3   0  24   0  18   0]\n",
      " [  0   0   0   1   0  11   0   1   0  21   2  15]\n",
      " [  0   1   0   0   2   1   1   0   7   1  27   3]\n",
      " [  0   0   0   0   0   4   0   0   0   3   0   8]]\n"
     ]
    }
   ],
   "source": [
    "nb_confusion_matrix = metrics.confusion_matrix(predicted_nb, expected_nb)\n",
    "print(nb_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_dt = y_test\n",
    "predicted_dt = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.810562934851\n"
     ]
    }
   ],
   "source": [
    "dt_accuracy = np.sum(predicted_dt==expected_dt)/len(expected_dt)\n",
    "print(dt_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation Accuracy, Check for Over/Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "dt_scores = cross_val_score(dt, X_train, y_train)\n",
    "print(\"%0.2f (+/- %0.2f)\" % (dt_scores.mean(), dt_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[376  53  34   0   0   0   0   0   0   0   1   1]\n",
      " [109 357  77   0   0   1   1   0   1   0   1   0]\n",
      " [ 11  58 309   0   0   1   0   0   0   1   0   0]\n",
      " [  0   0   0 407  86   0   0   1   1   1   1   1]\n",
      " [  0   0   0  97 468   0   1   0   1   0   1   0]\n",
      " [  0   0   0   0   0 539   0   0   0   0   0   0]\n",
      " [  0   0   0   3   1   1  17   1   0   0   4   1]\n",
      " [  0   0   0   0   0   0   2   8   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0   0  22   0   8   0]\n",
      " [  0   0   0   1   0   0   0   0   0  13   0  10]\n",
      " [  0   3   0   0   0   2   2   0   7   1  33   0]\n",
      " [  0   0   0   0   0   1   0   0   0   9   0  14]]\n"
     ]
    }
   ],
   "source": [
    "dt_confusion_matrix = metrics.confusion_matrix(predicted_dt, expected_dt)\n",
    "print(dt_confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected_lsvc = y_test\n",
    "predicted_lsvc = lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.944339025933\n"
     ]
    }
   ],
   "source": [
    "lsvc_accuracy = np.sum(predicted_lsvc==expected_lsvc)/len(expected_lsvc)\n",
    "print(lsvc_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Cross Validation Accuracy, Check for Over/Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94 (+/- 0.10)\n"
     ]
    }
   ],
   "source": [
    "# Adding a 10-fold cross-validation score, similar to the experiment \n",
    "# parameters in the paper\n",
    "lsvc_scores = cross_val_score(lsvc, X_train, y_train, cv=10)\n",
    "print(\"%0.2f (+/- %0.2f)\" % (lsvc_scores.mean(), lsvc_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[492  19  13   0   0   0   0   0   0   0   2   0]\n",
      " [  0 450  29   3   0   1   6   1   0   0   5   1]\n",
      " [  4   2 378   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0 435  40   0   2   1   0   0   1   0]\n",
      " [  0   0   0  70 516   0   0   0   0   0   1   0]\n",
      " [  0   0   0   0   0 544   0   0   0   0   2   0]\n",
      " [  0   0   0   0   0   0  15   8   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  20   1  11   0]\n",
      " [  0   0   0   0   0   0   0   0   0  23   0  20]\n",
      " [  0   0   0   0   0   0   0   0  12   0  27   3]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   3]]\n"
     ]
    }
   ],
   "source": [
    "svm_confusion_matrix = metrics.confusion_matrix(predicted_svm, expected_svm)\n",
    "print(svm_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
